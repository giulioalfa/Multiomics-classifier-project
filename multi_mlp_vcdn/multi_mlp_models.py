# -*- coding: utf-8 -*-
"""mlp_gcn_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16nW_EeQ8URtU2LTlqvnuz24Pg-Fm9vkV
"""

import torch.nn as nn
import torch
import torch.nn.functional as F

def xavier_init(m):
    if type(m) == nn.Linear:
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
           m.bias.data.fill_(0.0)
# architecture of multi-layer perceptron architecture and forward function
class MLP(nn.Module):
    def __init__(self, n_classes, n_inputs):
        super(MLP,self).__init__()
        self.n_classes = n_classes
        self.n_inputs = n_inputs
        # number of hidden nodes in each layer (512)
        hidden_1 = 512
        hidden_2 = 512
        # linear layer (784 -> hidden_1)
        self.fc1 = nn.Linear(n_inputs, 512)
        nn.init.xavier_normal_(self.fc1.weight)
        # linear layer (n_hidden -> hidden_2)
        self.fc2 = nn.Linear(512,512)
        nn.init.xavier_normal_(self.fc2.weight)
        # linear layer (n_hidden -> 10)
        self.fc3 = nn.Linear(512, n_classes)
        nn.init.xavier_normal_(self.fc3.weight)
        # dropout layer (p=0.2)
        # dropout prevents overfitting of data
        self.droput = nn.Dropout(0.2)

    def forward(self, x):
        # flatten image input
        x = x.view(-1, self.n_inputs)
        # add hidden layer, with relu activation function
        x = F.relu(self.fc1(x))
        # add dropout layer
        x = self.droput(x)
          # add hidden layer, with relu activation function
        x = F.relu(self.fc2(x))
        # add dropout layer
        x = self.droput(x)
        # add output layer
        x = self.fc3(x)
        return x
           
#architecture of the labels distributions integration, the so called VCDN of moronet, and forward function
class VCDN(nn.Module):
    def __init__(self, num_view, num_cls, hvcdn_dim):
        super().__init__()
        self.num_cls = num_cls
        self.model = nn.Sequential(
            nn.Linear(pow(num_cls, num_view), hvcdn_dim),
            nn.LeakyReLU(0.25),
            nn.Linear(hvcdn_dim, num_cls)
        )
        self.model.apply(xavier_init)
        
    def forward(self, in_list):
        num_view = len(in_list)
        for i in range(num_view):
            in_list[i] = torch.sigmoid(in_list[i])
        x = torch.reshape(torch.matmul(in_list[0].unsqueeze(-1), in_list[1].unsqueeze(1)),(-1,pow(self.num_cls,2),1))
        for i in range(2,num_view):
            x = torch.reshape(torch.matmul(x, in_list[i].unsqueeze(1)),(-1,pow(self.num_cls,i+1),1))
        vcdn_feat = torch.reshape(x, (-1,pow(self.num_cls,num_view)))
        output = self.model(vcdn_feat)

        return output


def init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hc, gcn_dopout=0.5):
    model_dict = {}
    for i in range(num_view):
        model_dict["N{:}".format(i+1)] = MLP(num_class, dim_list[i])     
    if num_view >= 2:
        model_dict["C"] = VCDN(num_view, num_class, dim_hc)
    print(model_dict.keys())
    return model_dict

def init_optim(num_view, model_dict, lr_e=1e-4, lr_c=1e-4):
    optim_dict = {}
    for i in range(num_view):
        optim_dict["N{:}".format(i+1)] = torch.optim.Adam(
                list(model_dict["N{:}".format(i+1)].parameters()), 
                lr=lr_e)
      
    if num_view >= 2:
        optim_dict["C"] = torch.optim.Adam(model_dict["C"].parameters(), lr=lr_c)
    return optim_dict
